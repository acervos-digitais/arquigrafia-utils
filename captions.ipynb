{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Captioning / Scene Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "from openai import OpenAI\n",
    "from os import listdir, makedirs, path\n",
    "from PIL import Image as PImage\n",
    "from transformers import AutoModel, AutoTokenizer, pipeline\n",
    "\n",
    "from envars import OPENAI_API_KEY\n",
    "\n",
    "ARQUI_IMAGE_URL = \"https://www.arquigrafia.org.br/arquigrafia-images/IDID_view.jpg\"\n",
    "\n",
    "IMAGES_IN_PATH = \"../../imgs/arquigrafia\"\n",
    "\n",
    "OUT_PATH = \"./metadata/json/captions\"\n",
    "makedirs(OUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SF_MODEL_NAME = \"Salesforce/blip-image-captioning-large\"\n",
    "blip_pipe = pipeline(\"image-to-text\", model=SF_MODEL_NAME, device=\"cuda\", torch_dtype=torch.float16)\n",
    "\n",
    "VIT_MODEL_NAME = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "vit_pipe = pipeline(\"image-to-text\", model=VIT_MODEL_NAME, device=\"cuda\", torch_dtype=torch.float16)\n",
    "\n",
    "POS_MODEL_NAME = \"QCRI/bert-base-multilingual-cased-pos-english\"\n",
    "pos_pipe = pipeline(model=POS_MODEL_NAME, device=\"cuda\")\n",
    "\n",
    "ENPT_MODEL_NAME = \"Helsinki-NLP/opus-mt-tc-big-en-pt\"\n",
    "ENPT_PIPELINE = pipeline(model=ENPT_MODEL_NAME, device=\"cuda\")\n",
    "\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CAP_MODEL_NAME = \"openbmb/MiniCPM-V-2\"\n",
    "CAP_MODEL_REV = \"187851962daa9b63072d40ec802f597b71bff532\"\n",
    "\n",
    "BINARIES = [\n",
    "  [\"horizontal\", \"vertical\"],\n",
    "  [\"translucent\", \"opaque\"],\n",
    "  [\"symmetric\", \"asymmetric\"],\n",
    "  [\"complex\", \"simple\"],\n",
    "  [\"internal\", \"external\"],\n",
    "  [\"open\", \"closed\"],\n",
    "]\n",
    "\n",
    "CAP_COND = [\n",
    "  {'role': 'user', 'content': \"The following image is a picture taken in Brazil.\"},\n",
    "  {'role': 'user', 'content': \"Give a short, precise, terse and objective description of the image without using superlatives.\"},\n",
    "  {'role': 'user', 'content': \"Don't mention sports or winter.\"},\n",
    "  {'role': 'user', 'content': \"Describe the image using only 8 nouns.\"},\n",
    "]\n",
    "\n",
    "CAP_MODEL = {\n",
    "  \"model\": AutoModel.from_pretrained(CAP_MODEL_NAME, revision=CAP_MODEL_REV, trust_remote_code=True, torch_dtype=torch.bfloat16).to(\"cuda\", dtype=torch.bfloat16),\n",
    "  \"pre\": AutoTokenizer.from_pretrained(CAP_MODEL_NAME, revision=CAP_MODEL_REV, trust_remote_code=True),\n",
    "  \"chat\": CAP_COND\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_caption_qa(img, model):\n",
    "  caption, _, _ = model[\"model\"].chat(\n",
    "    image=img,\n",
    "    msgs=model[\"chat\"],\n",
    "    max_length=32,\n",
    "    context=None,\n",
    "    tokenizer=model[\"pre\"],\n",
    "    sampling=True,\n",
    "    temperature=0.1\n",
    "  )\n",
    "  caption += \".\"\n",
    "  caption = caption[:caption.find(\".\") + 1]\n",
    "  caption = caption[:caption.find(\", possibly\")]\n",
    "  return \"Picture of \" + caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_caption_pipeline(img, pipe):\n",
    "  caption = pipe(img, max_new_tokens=200)[0][\"generated_text\"].lower()\n",
    "\n",
    "  pos = pos_pipe(caption)\n",
    "\n",
    "  nouns = []\n",
    "  for o in pos:\n",
    "    if o[\"entity\"].startswith(\"NN\"):\n",
    "      if o[\"word\"].startswith(\"#\") and len(nouns) > 1:\n",
    "        nouns[-1] = nouns[-1] + o[\"word\"].replace(\"#\", \"\")\n",
    "      elif not o[\"word\"].startswith(\"#\"):\n",
    "        nouns.append(o[\"word\"])\n",
    "\n",
    "  return \"Picture of \" + \", \".join(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_gpt_caption(cap):\n",
    "  return cap.strip().lower().replace(\"english: \", \"\").replace(\"portuguese: \", \"\")\n",
    "\n",
    "def run_caption_gpt(img_url, client):\n",
    "  LSEP = \"SEPARATOR\"\n",
    "  CAP_PREFIX = [\"Picture of \", \"Imagem de \"]\n",
    "\n",
    "  response = client.chat.completions.create(\n",
    "    #model=\"gpt-4o-mini\",\n",
    "    model=\"gpt-4o-2024-08-06\",\n",
    "    messages=[{\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"Whatâ€™s in this image? Answer using only nouns. Answer in english and portuguese.\"},\n",
    "        {\"type\": \"text\", \"text\": f\"Separate english and portuguese descriptions with the word {LSEP}\"},\n",
    "        {\"type\": \"image_url\", \"image_url\": {\"url\": img_url,},\n",
    "        },\n",
    "      ],\n",
    "    }],\n",
    "    max_tokens=200,\n",
    "  )\n",
    "\n",
    "  caps = response.choices[0].message.content.split(LSEP)\n",
    "  return tuple([p + clean_gpt_caption(c) for p,c in zip(CAP_PREFIX, caps)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin2float(b, b0, b1):\n",
    "  if b0 in b:\n",
    "    return 0.0\n",
    "  elif b1 in b:\n",
    "    return 1.0\n",
    "  elif \"neither\" in b:\n",
    "    return 0.5\n",
    "  else:\n",
    "    print(\"b2f error: \", b)\n",
    "    return 0.5\n",
    "\n",
    "def run_binaries(img, model, bins):\n",
    "  bin_results = {}\n",
    "  for b in bins:\n",
    "    chat = [{\n",
    "      \"role\": \"user\",\n",
    "      \"content\": f\"Is the architecture pictured in the image more {b[0]}, {b[1]} or neither? Answer using only the words {b[0]}, {b[1]} or neither\"\n",
    "    }]\n",
    "    response, _, _ = model[\"model\"].chat(\n",
    "      image=img,\n",
    "      msgs=chat,\n",
    "      max_length=4,\n",
    "      context=None,\n",
    "      tokenizer=model[\"pre\"],\n",
    "      sampling=True,\n",
    "      temperature=0.005\n",
    "    )\n",
    "\n",
    "    response_list = ' '.join(response.split()).split()\n",
    "    if len(response_list) != 1:\n",
    "      print(\"wtf\", response_list)\n",
    "    bin_results[\"/\".join(b)] = bin2float(response_list[0].lower(), b[0], b[1])\n",
    "\n",
    "  return bin_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "input_files = sorted([f for f in listdir(IMAGES_IN_PATH) if f.endswith(\"jpg\")])\n",
    "\n",
    "for io_file in input_files[:4096]:\n",
    "  img_id = io_file.replace(\".jpg\", \"\")\n",
    "  input_file_path = path.join(IMAGES_IN_PATH, io_file)\n",
    "  output_file_path = path.join(OUT_PATH, io_file.replace(\".jpg\", \".json\"))\n",
    "\n",
    "  if path.isfile(output_file_path):\n",
    "    continue\n",
    "\n",
    "  print(IMAGES_IN_PATH, io_file)\n",
    "\n",
    "  image = PImage.open(input_file_path).convert(\"RGB\")\n",
    "\n",
    "  image_captions = {}\n",
    "  image_captions[\"pt\"] = {}\n",
    "\n",
    "  image_captions[\"en\"] = {\n",
    "    \"cpm\": run_caption_qa(image, CAP_MODEL),\n",
    "    \"blip\": run_caption_pipeline(image, blip_pipe),\n",
    "    \"vit\": run_caption_pipeline(image, vit_pipe),\n",
    "  }\n",
    "\n",
    "  for k,v in image_captions[\"en\"].items():\n",
    "    to_pt = \">>por<< \" + v\n",
    "    image_captions[\"pt\"][k] = ENPT_PIPELINE(to_pt)[0][\"translation_text\"]\n",
    "\n",
    "  try:\n",
    "    gpt_cap = run_caption_gpt(ARQUI_IMAGE_URL.replace(\"IDID\", img_id), openai_client)\n",
    "    image_captions[\"en\"][\"gpt\"], image_captions[\"pt\"][\"gpt\"] = gpt_cap\n",
    "  except:\n",
    "    print(img_id, gpt_cap)\n",
    "  else:\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as of:\n",
    "      json.dump(image_captions, of, sort_keys=True, separators=(',',':'), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Process: Create output json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from export_utils import export_objs_caps\n",
    "\n",
    "OBJECTS_PATH = \"./metadata/json/objects\"\n",
    "CAPTIONS_PATH = \"./metadata/json/captions\"\n",
    "OBJECTS_DB_FILE_PATH = \"./metadata/json/objects.json\"\n",
    "\n",
    "export_objs_caps(OBJECTS_PATH, CAPTIONS_PATH, OBJECTS_DB_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Process: Create separate json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from export_utils import export_all_captions, export_by_keys\n",
    "\n",
    "CAPTIONS_PATH = \"./metadata/json/captions\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\"captions\"]\n",
    "export_by_keys(CAPTIONS_PATH, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_all_captions(CAPTIONS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST: EN/PT Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHRASES = [\n",
    "  \"I like to eat rice.\",\n",
    "  \"Tom tried to stab me.\",\n",
    "  \"He has been to Hawaii several times.\",\n",
    "  \"The image features a white house with black trim, windows on the front and side walls.\",\n",
    "  \"This image features a modern, open-concept living space with an eye-catching staircase and various furniture pieces.\",\n",
    "  \"The image depicts an interior space with a staircase, furniture such as chairs and tables.\",\n",
    "  \"The image showcases a modern building with glass walls, concrete stairs leading to it and greenery surrounding the area.\",\n",
    "  \"The image shows a view through glass panes, revealing indoor furniture and plants outside.\",\n",
    "  \"The image is of a modern building with large windows and columns.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ENPT_MODEL_NAME = \"Helsinki-NLP/opus-mt-tc-big-en-pt\"\n",
    "ENPT_PIPELINE = pipeline(model=ENPT_MODEL_NAME, device=\"cuda\")\n",
    "\n",
    "for p in PHRASES:\n",
    "  print(ENPT_PIPELINE(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST: Description Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from os import listdir, path\n",
    "from PIL import Image as PImage\n",
    "\n",
    "IMAGES_IN_PATH = \"../../imgs/arquigrafia\"\n",
    "INPUT_FILES = sorted([f for f in listdir(IMAGES_IN_PATH) if f.endswith(\"jpg\")])\n",
    "\n",
    "io_file = INPUT_FILES[10]\n",
    "io_file = \"10026.jpg\"\n",
    "input_file_path = path.join(IMAGES_IN_PATH, io_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = PImage.open(input_file_path).convert(\"RGB\")\n",
    "cap = run_caption(image, CAP_MODEL)\n",
    "\n",
    "to_pt = \">>por<< \" + cap\n",
    "cap_pt = ENPT_PIPELINE(to_pt)[0][\"translation_text\"]\n",
    "\n",
    "display(image)\n",
    "cap, cap_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST: Caption Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from os import listdir, path\n",
    "from PIL import Image as PImage\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "IMAGES_IN_PATH = \"../../imgs/arquigrafia\"\n",
    "INPUT_FILES = sorted([f for f in listdir(IMAGES_IN_PATH) if f.endswith(\"jpg\")])\n",
    "\n",
    "io_file = INPUT_FILES[10]\n",
    "io_file = \"10027.jpg\"\n",
    "io_file = \"10000.jpg\"\n",
    "input_file_path = path.join(IMAGES_IN_PATH, io_file)\n",
    "image = PImage.open(input_file_path).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SF_MODEL_NAME = \"Salesforce/blip-image-captioning-large\"\n",
    "blip_pipe = pipeline(\"image-to-text\", model=SF_MODEL_NAME, device=\"cuda\", torch_dtype=torch.float16)\n",
    "\n",
    "VIT_MODEL_NAME = \"nlpconnect/vit-gpt2-image-captioning\"\n",
    "vit_pipe = pipeline(\"image-to-text\", model=VIT_MODEL_NAME, device=\"cuda\", torch_dtype=torch.float16)\n",
    "\n",
    "VIT2_MODEL_NAME = \"ydshieh/vit-gpt2-coco-en\"\n",
    "vit2_pipe = pipeline(\"image-to-text\", model=VIT2_MODEL_NAME, device=\"cuda\", torch_dtype=torch.float16)\n",
    "\n",
    "POS_MODEL_NAME = \"QCRI/bert-base-multilingual-cased-pos-english\"\n",
    "pos_pipe = pipeline(model=POS_MODEL_NAME, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blip_pipe(image, max_new_tokens=200),\\\n",
    "vit_pipe(image, max_new_tokens=200),\\\n",
    "vit2_pipe(image, max_new_tokens=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = blip_pipe(image, max_new_tokens=200)[0][\"generated_text\"]\n",
    "pos = pos_pipe(cap)\n",
    "cap,pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://platform.openai.com/docs/guides/vision\n",
    "# https://github.com/openai/openai-python\n",
    "\n",
    "from openai import OpenAI\n",
    "from msecrets import OPENAI_API_KEY\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  #model=\"gpt-4o-mini\",\n",
    "  model=\"gpt-4o-2024-08-06\",\n",
    "  messages=[{\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"Whatâ€™s in this image? Answer using only nouns. Answer in english and portuguese.\"},\n",
    "        {\"type\": \"text\", \"text\": \"Separate english and portuguese descriptions with the word SEPARATOR\"},\n",
    "        {\"type\": \"image_url\",\n",
    "          \"image_url\": {\"url\": \"https://www.arquigrafia.org.br/arquigrafia-images/10026_view.jpg\",},\n",
    "        },\n",
    "      ],\n",
    "    }],\n",
    "  max_tokens=200,\n",
    ")\n",
    "\n",
    "print(response.choices[0])\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST: Binaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from os import listdir, path\n",
    "from PIL import Image as PImage\n",
    "\n",
    "IMAGES_IN_PATH = \"../../imgs/arquigrafia\"\n",
    "INPUT_FILES = sorted([f for f in listdir(IMAGES_IN_PATH) if f.endswith(\"jpg\")])\n",
    "\n",
    "io_file = INPUT_FILES[100]\n",
    "input_file_path = path.join(IMAGES_IN_PATH, io_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "image = PImage.open(input_file_path).convert(\"RGB\")\n",
    "image_binaries = run_binaries(image, CAP_MODEL, BINARIES)\n",
    "\n",
    "display(image)\n",
    "image_binaries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
