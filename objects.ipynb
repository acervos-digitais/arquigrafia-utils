{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Object Detection / Captioning / Scene Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "\n",
    "from os import listdir, makedirs, path\n",
    "\n",
    "from PIL import Image as PImage\n",
    "\n",
    "from dominant_colors import get_dominant_colors, resize_PIL\n",
    "\n",
    "IMAGES_IN_PATH = \"../../imgs/arquigrafia\"\n",
    "\n",
    "OUT_PATH = \"./metadata/objects\"\n",
    "makedirs(OUT_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJS = [\n",
    "  {\n",
    "    \"minaret\": 0.25,\n",
    "    \"tower\": 0.6,\n",
    "    \"railing\": 0.4,\n",
    "    \"stair railing\": 0.41,\n",
    "    \"guard railing\": 0.4,\n",
    "    \"table\": 0.45,\n",
    "    \"desk\": 0.25,\n",
    "    \"chair\": 0.24,\n",
    "    \"inclined walkway\": 0.32,\n",
    "    \"sculpture\": 0.4,\n",
    "    \"painting\": 0.4,\n",
    "    \"vertical pillar\": 0.35,\n",
    "    \"stairs\": 0.4,\n",
    "    \"stoop steps\": 0.35,\n",
    "    \"stoop stairs\": 0.35,\n",
    "  },\n",
    "  {\n",
    "    \"window\": 0.2,\n",
    "    \"room door\": 0.25,\n",
    "    \"building door\": 0.22,\n",
    "    \"masonry\": 0.2,\n",
    "\n",
    "    \"concrete wall\": 0.2,\n",
    "    \"exposed concrete\": 0.2,\n",
    "    \"concrete structure\": 0.2,\n",
    "    \"poured concrete\": 0.2,\n",
    "  \n",
    "    \"glass window\": 0.2,\n",
    "    \"glass door\": 0.2,\n",
    "    \"mirror\": 0.2,\n",
    "  },\n",
    "  {\n",
    "    \"wood fence\": 0.3,\n",
    "    \"wood railing\": 0.35,\n",
    "    \"wood pilar\": 0.3,\n",
    "    \"wood door\": 0.21,\n",
    "    \"wood board\": 0.21,\n",
    "\n",
    "    \"metal fence\": 0.4,\n",
    "    \"metal railing\": 0.22,\n",
    "    \"wrought\": 0.2,\n",
    "  },\n",
    "  {\n",
    "    \"tree\": 0.2,\n",
    "    \"grass\": 0.2,\n",
    "    \"shrub\": 0.2,\n",
    "    \"bush\": 0.2,\n",
    "    \"flower\": 0.2,\n",
    "    \"vegetation\": 0.2,\n",
    "    \"greenery\": 0.2,\n",
    "  }\n",
    "]\n",
    "\n",
    "LABEL2LABEL = {\n",
    "  \"minaret\": \"tower\",\n",
    "  \"stair railing\": \"railing\",\n",
    "  \"guard railing\": \"railing\",\n",
    "  \"stoop steps\": \"stairs\",\n",
    "  \"stoop stairs\": \"stairs\",\n",
    "  \"desk\": \"table\",\n",
    "  \"room door\": \"building door\",\n",
    "\n",
    "  \"exposed concrete\": \"concrete wall\",\n",
    "  \"concrete structure\": \"concrete wall\",\n",
    "  \"poured concrete\": \"concrete wall\",\n",
    "\n",
    "  \"glass window\": \"mirror\",\n",
    "  \"glass door\": \"mirror\",\n",
    "\n",
    "  \"wood railing\": \"wood fence\",\n",
    "  \"wood pilar\":\"wood fence\",\n",
    "  \"wood door\": \"wood fence\",\n",
    "  \"wood board\": \"wood fence\",\n",
    "\n",
    "  \"metal fence\": \"wrought\",\n",
    "  \"metal railing\": \"wrought\",\n",
    "\n",
    "  \"tree\": \"greenery\",\n",
    "  \"grass\": \"greenery\",\n",
    "  \"shrub\": \"greenery\",\n",
    "  \"bush\": \"greenery\",\n",
    "  \"flower\": \"greenery\",\n",
    "  \"vegetation\": \"greenery\",\n",
    "}\n",
    "\n",
    "OBJS_LABELS_IN = [sorted(o.keys()) for o in OBJS]\n",
    "OBJS_LABELS_OUT = [[LABEL2LABEL.get(l, l) for l in oli] for oli in OBJS_LABELS_IN]\n",
    "OBJS_THOLDS = [[OBJS[i][k] for k in oli] for i,oli in enumerate(OBJS_LABELS_IN)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer, pipeline\n",
    "\n",
    "CAP_MODEL_NAME = \"openbmb/MiniCPM-V-2\"\n",
    "CAP_MODEL_REV = \"187851962daa9b63072d40ec802f597b71bff532\"\n",
    "\n",
    "CAP_COND = [\n",
    "  {'role': 'user', 'content': \"The following image is a picture taken in Brazil.\"},\n",
    "  {'role': 'user', 'content': \"Give a short, precise, terse and objective description of the image without using superlatives.\"},\n",
    "  {'role': 'user', 'content': \"Don't mention sports or winter.\"},\n",
    "  {'role': 'user', 'content': \"Describe the image using only 8 nouns.\"},\n",
    "]\n",
    "\n",
    "CAP_MODEL = {\n",
    "  \"model\": AutoModel.from_pretrained(CAP_MODEL_NAME, revision=CAP_MODEL_REV, trust_remote_code=True, torch_dtype=torch.bfloat16).to(\"cuda\", dtype=torch.bfloat16),\n",
    "  \"pre\": AutoTokenizer.from_pretrained(CAP_MODEL_NAME, revision=CAP_MODEL_REV, trust_remote_code=True),\n",
    "  \"chat\": CAP_COND\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENPT_MODEL_NAME = \"Helsinki-NLP/opus-mt-tc-big-en-pt\"\n",
    "ENPT_PIPELINE = pipeline(model=ENPT_MODEL_NAME, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_caption(img, model):\n",
    "  caption, _, _ = model[\"model\"].chat(\n",
    "    image=img,\n",
    "    msgs=model[\"chat\"],\n",
    "    max_length=32,\n",
    "    context=None,\n",
    "    tokenizer=model[\"pre\"],\n",
    "    sampling=True,\n",
    "    temperature=0.1\n",
    "  )\n",
    "  caption += \".\"\n",
    "  caption = caption[:caption.find(\".\") + 1]\n",
    "  caption = caption[:caption.find(\", possibly\")]\n",
    "  return \"Picture of \" + caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Owlv2Processor, Owlv2ForObjectDetection\n",
    "\n",
    "OBJ_TARGET_SIZE = torch.Tensor([500, 500])\n",
    "OBJ_MODEL = \"google/owlv2-base-patch16-ensemble\"\n",
    "\n",
    "obj_model = Owlv2ForObjectDetection.from_pretrained(OBJ_MODEL).to(\"cuda\")\n",
    "obj_processor = Owlv2Processor.from_pretrained(OBJ_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_px_to_pct(box, img_w, img_h, model_dims):\n",
    "  scale_factor = torch.tensor([max(img_w, img_h) / img_w , max(img_w, img_h) / img_h])\n",
    "  return [round(x, 4) for x in (box.cpu().reshape(2, -1) / model_dims * scale_factor).reshape(-1).tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_object_detection(img, obj_labels_in, obj_labels_out, obj_tholds):\n",
    "  input = obj_processor(text=obj_labels_in, images=img, return_tensors=\"pt\").to(\"cuda\")\n",
    "  with torch.no_grad():\n",
    "    obj_out = obj_model(**input)\n",
    "\n",
    "  res = obj_processor.post_process_object_detection(outputs=obj_out, target_sizes=[OBJ_TARGET_SIZE])\n",
    "  slbs = zip(res[0][\"scores\"], res[0][\"labels\"], res[0][\"boxes\"])\n",
    "  iw, ih = img.size\n",
    "\n",
    "  # filter if box \"too large\" or \"too small\"\n",
    "  def good_thold_and_size(s, l, b):\n",
    "    box_pct = box_px_to_pct(b, iw, ih, OBJ_TARGET_SIZE)\n",
    "    box_width = box_pct[2] - box_pct[0]\n",
    "    box_height = box_pct[3] - box_pct[1]\n",
    "    good_min = box_width > 0.05 and box_height > 0.05\n",
    "    good_max = box_width < 0.8 or box_height < 0.8\n",
    "    return good_min and good_max and s > obj_tholds[l.item()]\n",
    "\n",
    "  detected_objs = [{\"score\": s, \"label\": obj_labels_out[l.item()], \"box\": box_px_to_pct(b, iw, ih, OBJ_TARGET_SIZE)} for s,l,b in slbs if good_thold_and_size(s, l, b)]\n",
    "\n",
    "  # only keep the box with highest score per object\n",
    "  detected_objs_boxes = {}\n",
    "  high_score = {}\n",
    "\n",
    "  for o in detected_objs:\n",
    "    ol = o[\"label\"]\n",
    "    if (ol not in detected_objs_boxes) or (o[\"score\"] > high_score[ol]):\n",
    "      detected_objs_boxes[ol] = o[\"box\"]\n",
    "      high_score[ol] = o[\"score\"]\n",
    "\n",
    "  return detected_objs_boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "input_files = sorted([f for f in listdir(IMAGES_IN_PATH) if f.endswith(\"jpg\")])\n",
    "\n",
    "for io_file in input_files[:1024]:\n",
    "  input_file_path = path.join(IMAGES_IN_PATH, io_file)\n",
    "  output_file_path = path.join(OUT_PATH, io_file.replace(\".jpg\", \".json\"))\n",
    "\n",
    "  if path.isfile(output_file_path):\n",
    "    continue\n",
    "\n",
    "  if int(io_file.replace(\".jpg\", \"\")) % 50 == 0:\n",
    "    print(IMAGES_IN_PATH, io_file)\n",
    "\n",
    "  image = PImage.open(input_file_path).convert(\"RGB\")\n",
    "\n",
    "  rgb_by_count, rgb_by_hls = get_dominant_colors(resize_PIL(image))\n",
    "\n",
    "  image_data = {}\n",
    "  image_data[\"caption\"] = {}\n",
    "  image_data[\"caption\"][\"en\"] = run_caption(image, CAP_MODEL)\n",
    "  to_pt = \">>por<< \" + image_data[\"caption\"][\"en\"]\n",
    "  image_data[\"caption\"][\"pt\"] = ENPT_PIPELINE(to_pt)[0][\"translation_text\"]\n",
    "  image_data[\"boxes\"] = run_object_detection(image, OBJS_LABELS_IN, OBJS_LABELS_OUT, OBJS_THOLDS)\n",
    "\n",
    "  image_data[\"dominant_color\"] = {\n",
    "    \"by_count\": [int(v) for v in rgb_by_count[0]],\n",
    "    \"by_hue\": [int(v) for v in rgb_by_hls[0]],\n",
    "    \"palette\": [[int(v) for v in c] for c in rgb_by_hls[:4]],\n",
    "  }\n",
    "\n",
    "  with open(output_file_path, \"w\", encoding=\"utf-8\") as of:\n",
    "    json.dump(image_data, of, sort_keys=True, separators=(',',':'), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Process: Create output json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from os import listdir, path\n",
    "\n",
    "from dominant_colors import hls_order_from_rgb255\n",
    "\n",
    "CAPTIONS_PATH = \"./metadata/objects\"\n",
    "OBJECTS_DB_FILE_PATH = \"./metadata/objects.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename -> image info\n",
    "img_data = {}\n",
    "\n",
    "# obj name -> image name\n",
    "obj_data = {}\n",
    "\n",
    "# image name -> color order key\n",
    "color_key = {}\n",
    "\n",
    "input_files = sorted([f for f in listdir(CAPTIONS_PATH) if f.endswith(\"json\")])\n",
    "\n",
    "for io_file in input_files:\n",
    "  input_file_path = path.join(CAPTIONS_PATH, io_file)\n",
    "  with open(input_file_path, \"r\", encoding=\"utf8\") as f:\n",
    "    id = int(io_file.replace(\".json\", \"\"))\n",
    "    img_data[id] = json.load(f)\n",
    "\n",
    "    for l in img_data[id][\"boxes\"].keys():\n",
    "      obj_data[l] = obj_data.get(l, []) + [id]\n",
    "\n",
    "    color_key[id] = hls_order_from_rgb255(img_data[id][\"dominant_color\"][\"by_hue\"])\n",
    "\n",
    "# order each object's file list by color order\n",
    "for k in obj_data.keys():\n",
    "  obj_data[k] = sorted(obj_data[k], key=lambda x: color_key[x])\n",
    "\n",
    "out_data = {\n",
    "  \"objects\": obj_data,\n",
    "  \"images\": img_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(OBJECTS_DB_FILE_PATH, \"w\", encoding=\"utf8\") as f:\n",
    "  json.dump(out_data, f, separators=(',',':'), sort_keys=True, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST: boxes from JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from os import path\n",
    "from PIL import Image as PImage, ImageDraw as PImageDraw\n",
    "\n",
    "OBJECTS_DB_FILE_PATH = \"./metadata/objects.json\"\n",
    "IMAGES_PATH = \"../../imgs/arquigrafia\"\n",
    "\n",
    "with open(OBJECTS_DB_FILE_PATH, \"r\") as f:\n",
    "  json_data = json.load(f)\n",
    "  img_data = json_data[\"images\"]\n",
    "  obj_data = json_data[\"objects\"]\n",
    "\n",
    "for id, d in list(img_data.items())[:3]:\n",
    "  img_path = path.join(IMAGES_PATH, f\"{id}.jpg\")\n",
    "  img = PImage.open(img_path).convert(\"RGBA\")\n",
    "  iw,ih = img.size\n",
    "  draw = PImageDraw.Draw(img)\n",
    "  for _, (x0,y0,x1,y1) in d[\"boxes\"].items():\n",
    "    draw.rectangle(((x0*iw, y0*ih), (x1*iw, y1*ih)), outline=(255, 0, 0))\n",
    "  print(list(d[\"boxes\"].keys()), \"\\n\", d[\"caption\"])\n",
    "  display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST: EN -> PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PHRASES = [\n",
    "  \"I like to eat rice.\",\n",
    "  \"Tom tried to stab me.\",\n",
    "  \"He has been to Hawaii several times.\",\n",
    "  \"The image features a white house with black trim, windows on the front and side walls.\",\n",
    "  \"This image features a modern, open-concept living space with an eye-catching staircase and various furniture pieces.\",\n",
    "  \"The image depicts an interior space with a staircase, furniture such as chairs and tables.\",\n",
    "  \"The image showcases a modern building with glass walls, concrete stairs leading to it and greenery surrounding the area.\",\n",
    "  \"The image shows a view through glass panes, revealing indoor furniture and plants outside.\",\n",
    "  \"The image is of a modern building with large windows and columns.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ENPT_MODEL_NAME = \"Helsinki-NLP/opus-mt-tc-big-en-pt\"\n",
    "ENPT_PIPELINE = pipeline(model=ENPT_MODEL_NAME, device=\"cuda\")\n",
    "\n",
    "for p in PHRASES:\n",
    "  print(ENPT_PIPELINE(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST: Dominant Color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from os import listdir, path\n",
    "from PIL import Image as PImage\n",
    "\n",
    "from dominant_colors import get_dominant_colors, resize_PIL\n",
    "\n",
    "IMAGES_IN_PATH = \"../../imgs/arquigrafia\"\n",
    "INPUT_FILES = sorted([f for f in listdir(IMAGES_IN_PATH) if f.endswith(\"jpg\")])\n",
    "\n",
    "io_file = INPUT_FILES[0]\n",
    "input_file_path = path.join(IMAGES_IN_PATH, io_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = PImage.open(input_file_path).convert(\"RGB\")\n",
    "image_s = resize_PIL(image)\n",
    "rgb_by_count, rgb_by_hls = get_dominant_colors(image_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iw, ih = [(d // 2) * 2 for d in image_s.size]\n",
    "image_shape = (ih, iw, 3)\n",
    "ppc = int(ih * iw / len(rgb_by_count))\n",
    "\n",
    "count_np_image = np.array([ppc * [c] for c in rgb_by_count]).reshape(image_shape)\n",
    "hls_np_image = np.array([ppc * [c] for c in rgb_by_hls]).reshape(image_shape)\n",
    "\n",
    "display(image_s)\n",
    "display(PImage.fromarray(count_np_image))\n",
    "display(PImage.fromarray(hls_np_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjust Thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "\n",
    "from os import path, listdir, makedirs\n",
    "\n",
    "OBJECTS_DB_FILE_PATH = \"./metadata/objects.json\"\n",
    "IMAGES_PATH = \"../../imgs\"\n",
    "IMG_IN_DIR = \"arquigrafia\"\n",
    "\n",
    "with open(OBJECTS_DB_FILE_PATH, \"r\") as f:\n",
    "  json_data = json.load(f)\n",
    "  img_data = json_data[\"images\"]\n",
    "  obj_data = json_data[\"objects\"]\n",
    "\n",
    "print(obj_data.keys(), len(obj_data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tocopy = [\n",
    "  'inclined walkway',\n",
    "]\n",
    "\n",
    "for o in tocopy:\n",
    "  print(o, len(obj_data[o]), obj_data[o], \"\\n\")\n",
    "  img_out_dir = f\"test-{o.replace(' ', '-')}\"\n",
    "  img_out_dir_path = path.join(IMAGES_PATH, img_out_dir)\n",
    "  makedirs(img_out_dir_path, exist_ok=True)\n",
    "  for i in obj_data[o]:\n",
    "    img_in_path = path.join(IMAGES_PATH, IMG_IN_DIR, f\"{i}.jpg\")\n",
    "    shutil.copy2(img_in_path, img_out_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image as PImage, ImageDraw as PImageDraw, ImageFont as PImageFont\n",
    "\n",
    "MFONT = PImageFont.load_default(20)\n",
    "\n",
    "TEST_PATH = \"../../imgs/test-inclined-walkway\"\n",
    "\n",
    "OBJS = {\n",
    "  \"inclined walkway\": 0.41,\n",
    "}\n",
    "\n",
    "LABEL2LABEL = {}\n",
    "\n",
    "OBJS_LABELS_IN = sorted(OBJS.keys())\n",
    "OBJS_LABELS_OUT = [LABEL2LABEL.get(l, l) for l in OBJS_LABELS_IN]\n",
    "OBJS_THOLDS = [OBJS[k] for k in OBJS_LABELS_IN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_object_detection(img, obj_labels_in, obj_labels_out, obj_tholds):\n",
    "  input = obj_processor(text=obj_labels_in, images=img, return_tensors=\"pt\").to(\"cuda\")\n",
    "  with torch.no_grad():\n",
    "    obj_out = obj_model(**input)\n",
    "\n",
    "  res = obj_processor.post_process_object_detection(outputs=obj_out, target_sizes=[OBJ_TARGET_SIZE])\n",
    "  slbs = zip(res[0][\"scores\"], res[0][\"labels\"], res[0][\"boxes\"])\n",
    "  iw, ih = img.size\n",
    "\n",
    "  # filter if box \"too large\" or \"too small\"\n",
    "  def good_thold_and_size(s, l, b):\n",
    "    box_pct = box_px_to_pct(b, iw, ih, OBJ_TARGET_SIZE)\n",
    "    box_width = box_pct[2] - box_pct[0]\n",
    "    box_height = box_pct[3] - box_pct[1]\n",
    "    good_min = box_width > 0.05 and box_height > 0.05\n",
    "    good_max = box_width < 0.8 or box_height < 0.8\n",
    "    return good_min and good_max and s > obj_tholds[l.item()]\n",
    "\n",
    "  detected_objs = [{\"score\": s.item(), \"label\": obj_labels_out[l.item()], \"box\": box_px_to_pct(b, iw, ih, OBJ_TARGET_SIZE)} for s,l,b in slbs if good_thold_and_size(s, l, b)]\n",
    "\n",
    "  # only keep the box with highest score per object\n",
    "  detected_objs_boxes = {}\n",
    "  high_score = {}\n",
    "\n",
    "  for o in detected_objs:\n",
    "    ol = o[\"label\"]\n",
    "    if (ol not in detected_objs_boxes) or (o[\"score\"] > high_score[ol]):\n",
    "      detected_objs_boxes[ol] = o[\"box\"]\n",
    "      high_score[ol] = o[\"score\"]\n",
    "\n",
    "  return detected_objs #detected_objs_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppath = TEST_PATH\n",
    "input_files = sorted([f for f in listdir(ppath) if f.endswith(\"jpg\")])\n",
    "\n",
    "for io_file in input_files:\n",
    "  input_file_path = path.join(ppath, io_file)\n",
    "\n",
    "  image = PImage.open(input_file_path).convert(\"RGB\")\n",
    "  iw,ih = image.size\n",
    "  print(image.size)\n",
    "\n",
    "  objs = run_object_detection(image, OBJS_LABELS_IN, OBJS_LABELS_OUT, OBJS_THOLDS)\n",
    "  print([f'{o[\"label\"]}: {o[\"score\"]}' for o in objs])\n",
    "\n",
    "  draw = PImageDraw.Draw(image)\n",
    "  for o in objs:\n",
    "    (x0,y0,x1,y1) = o[\"box\"]\n",
    "    score, label = o[\"score\"], o[\"label\"]\n",
    "    draw.rectangle(((x0*iw, y0*ih), (x1*iw, y1*ih)), outline=(255, 0, 0), width=2)\n",
    "    draw.text((x0*iw, y0*ih + 20), f\"{round(score, 3)}\", (255, 255, 255), font=MFONT)\n",
    "    draw.text((x0*iw, y0*ih - 0), f\"{label}\", (255, 0, 0), font=MFONT)\n",
    "  display(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
