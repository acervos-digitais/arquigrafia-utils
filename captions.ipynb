{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Captioning / Scene Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from os import listdir, makedirs, path\n",
    "from PIL import Image as PImage\n",
    "\n",
    "from captions_models import Blip, Vit, CPM2, GPT4, EnPt\n",
    "\n",
    "from parameters import IMAGES_PATH, CAPTIONS_PATH\n",
    "\n",
    "makedirs(CAPTIONS_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "input_files = sorted([f for f in listdir(IMAGES_PATH) if f.endswith(\"jpg\")])\n",
    "\n",
    "for io_file in input_files[:4096]:\n",
    "  img_id = io_file.replace(\".jpg\", \"\")\n",
    "  input_file_path = path.join(IMAGES_PATH, io_file)\n",
    "  output_file_path = path.join(CAPTIONS_PATH, io_file.replace(\".jpg\", \".json\"))\n",
    "\n",
    "  if path.isfile(output_file_path):\n",
    "    continue\n",
    "\n",
    "  print(IMAGES_PATH, io_file)\n",
    "\n",
    "  image = PImage.open(input_file_path).convert(\"RGB\")\n",
    "\n",
    "  image_captions = {}\n",
    "  image_captions[\"pt\"] = {}\n",
    "\n",
    "  image_captions[\"en\"] = {\n",
    "    \"cpm\": CPM2.caption(image),\n",
    "    \"blip\": Blip.caption(image),\n",
    "    \"vit\": Vit.caption(image),\n",
    "  }\n",
    "\n",
    "  for k,txt in image_captions[\"en\"].items():\n",
    "    image_captions[\"pt\"][k] = EnPt.translate(txt)\n",
    "\n",
    "  try:\n",
    "    gpt_cap = GPT4.caption(img_id)\n",
    "    image_captions[\"en\"][\"gpt\"], image_captions[\"pt\"][\"gpt\"] = gpt_cap\n",
    "  except:\n",
    "    print(img_id, gpt_cap)\n",
    "  else:\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as of:\n",
    "      json.dump(image_captions, of, sort_keys=True, separators=(',',':'), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = sorted([f for f in listdir(IMAGES_PATH) if f.endswith(\"jpg\")])\n",
    "\n",
    "no_gpt = []\n",
    "\n",
    "for io_file in input_files[:4096]:\n",
    "  input_file_path = path.join(IMAGES_PATH, io_file)\n",
    "  output_file_path = path.join(CAPTIONS_PATH, io_file.replace(\".jpg\", \".json\"))\n",
    "\n",
    "  if path.isfile(output_file_path):\n",
    "    with open(output_file_path, \"r\", encoding=\"utf8\") as capf:\n",
    "      fcaps = json.load(capf)\n",
    "      if \"gpt\" not in fcaps[\"en\"]:\n",
    "        no_gpt.append(io_file)\n",
    "  else:\n",
    "    no_gpt.append(io_file)\n",
    "\n",
    "len(no_gpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean up GPT Captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = sorted([f for f in listdir(CAPTIONS_PATH) if f.endswith(\"json\")])\n",
    "\n",
    "for io_file in input_files:\n",
    "  file_path = path.join(CAPTIONS_PATH, io_file)\n",
    "\n",
    "  with open(file_path, \"r\", encoding=\"utf8\") as inf:\n",
    "    fcaps = json.load(inf)\n",
    "    ntw = False\n",
    "\n",
    "    for l in [\"en\", \"pt\"]:\n",
    "      if \"gpt\" not in fcaps[l]:\n",
    "        ntw = True\n",
    "        fcaps[l][\"gpt\"] = fcaps[l][\"cpm\"]\n",
    "\n",
    "      if \"english: \" in fcaps[l][\"gpt\"] or \"portuguese: \" in fcaps[l][\"gpt\"]:\n",
    "        ntw = True\n",
    "        fcaps[l][\"gpt\"] = fcaps[l][\"gpt\"].replace(\"english: \", \"\").replace(\"portuguese: \", \"\")\n",
    "\n",
    "      if fcaps[l][\"gpt\"].lower() == \"picture of \" or fcaps[l][\"gpt\"].lower() == \"imagem de \":\n",
    "        ntw = True\n",
    "        fcaps[l][\"gpt\"] = fcaps[l][\"cpm\"]\n",
    "\n",
    "  if ntw:\n",
    "    print(io_file)\n",
    "    with open(file_path, \"w\", encoding=\"utf8\") as outf:\n",
    "      json.dump(fcaps, outf, sort_keys=True, separators=(',',':'), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_files = sorted([f for f in listdir(CAPTIONS_PATH) if f.endswith(\"json\")])\n",
    "\n",
    "to_remove_en = [\n",
    "  \"sure, here are the nouns\",\n",
    "  \"sure, here are the\",\n",
    "  \"sure here are the nouns\",\n",
    "  \"sure, here you go\",\n",
    "  \"sure here you go\",\n",
    "  \" sure,\",\n",
    "  \" sure\",\n",
    "  \" nouns in\",\n",
    "  \" nouns\",\n",
    "  \" certainly!\",\n",
    "  \" certainly\",\n",
    "  \" certainly,\",\n",
    "  \"**english**\",\n",
    "]\n",
    "\n",
    "to_remove_pt = [\n",
    "  \"**portuguese**\",\n",
    "  \"**português**\",\n",
    "  \"**\",\n",
    "  \" substantivos\",\n",
    "]\n",
    "\n",
    "for io_file in input_files:\n",
    "  file_path = path.join(CAPTIONS_PATH, io_file)\n",
    "\n",
    "  with open(file_path, \"r\", encoding=\"utf8\") as inf:\n",
    "    fcaps = json.load(inf)\n",
    "    gpt_en = fcaps[\"en\"][\"gpt\"].replace(\"\\n\", \"\").replace(\"  \", \" \").replace(\".\", \"\").replace(\":\", \"\")\n",
    "    gpt_pt = fcaps[\"pt\"][\"gpt\"].replace(\"\\n\", \"\").replace(\"  \", \" \").replace(\".\", \"\").replace(\":\", \"\")\n",
    "\n",
    "    for tr in to_remove_en:\n",
    "      if tr in gpt_en:\n",
    "        print(io_file, tr, \"\\n\\t\", gpt_en, \"\\n\\t\", gpt_en.replace(tr, \"\"))\n",
    "        gpt_en = gpt_en.replace(tr, \"\")\n",
    "    fcaps[\"en\"][\"gpt\"] = gpt_en\n",
    "\n",
    "    for tr in to_remove_pt:\n",
    "      if tr in gpt_pt:\n",
    "        print(io_file, tr, \"\\n\\t\", gpt_pt, \"\\n\\t\", gpt_pt.replace(tr, \"\"))\n",
    "        gpt_pt = gpt_pt.replace(tr, \"\")\n",
    "    fcaps[\"pt\"][\"gpt\"] = gpt_pt\n",
    "\n",
    "  #with open(file_path, \"w\", encoding=\"utf8\") as outf:\n",
    "    #json.dump(fcaps, outf, sort_keys=True, separators=(',',':'), ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Process: Create output json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from export_utils import export_objs_caps\n",
    "from parameters import OBJECTS_PATH, CAPTIONS_PATH, DB_FILE_PATH\n",
    "\n",
    "export_objs_caps(OBJECTS_PATH, CAPTIONS_PATH, DB_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Process: Create separate json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from export_utils import export_by_keys\n",
    "from parameters import CAPTIONS_PATH\n",
    "\n",
    "keys = [\"captions\"]\n",
    "export_by_keys(CAPTIONS_PATH, keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from export_utils import export_all_captions\n",
    "from parameters import CAPTIONS_PATH\n",
    "\n",
    "export_all_captions(CAPTIONS_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST: EN/PT Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.EnPt import EnPt\n",
    "\n",
    "PHRASES = [\n",
    "  \"I like to eat rice.\",\n",
    "  \"Tom tried to stab me.\",\n",
    "  \"He has been to Hawaii several times.\",\n",
    "  \"The image features a white house with black trim, windows on the front and side walls.\",\n",
    "  \"This image features a modern, open-concept living space with an eye-catching staircase and various furniture pieces.\",\n",
    "  \"The image depicts an interior space with a staircase, furniture such as chairs and tables.\",\n",
    "  \"The image showcases a modern building with glass walls, concrete stairs leading to it and greenery surrounding the area.\",\n",
    "  \"The image shows a view through glass panes, revealing indoor furniture and plants outside.\",\n",
    "  \"The image is of a modern building with large windows and columns.\"\n",
    "]\n",
    "\n",
    "for p in PHRASES:\n",
    "  print(EnPt.translate(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST: Description Translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from PIL import Image as PImage\n",
    "\n",
    "from models.Blip import Blip\n",
    "from models.Vit import Vit\n",
    "from models.EnPt import EnPt\n",
    "\n",
    "from parameters import IMAGES_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file_path = path.join(IMAGES_PATH, \"10026.jpg\")\n",
    "image = PImage.open(input_file_path).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = Blip.caption(image)\n",
    "cap_pt = EnPt.translate(cap)\n",
    "\n",
    "display(image)\n",
    "cap, cap_pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST: Caption Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from os import path\n",
    "from PIL import Image as PImage\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration, LlavaNextForConditionalGeneration\n",
    "\n",
    "from parameters import IMAGES_PATH\n",
    "\n",
    "io_file = \"10027.jpg\"\n",
    "io_file = \"10000.jpg\"\n",
    "\n",
    "input_file_path = path.join(IMAGES_PATH, io_file)\n",
    "image = PImage.open(input_file_path).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_xtuner_llava_model(model_name):\n",
    "  processor = AutoProcessor.from_pretrained(model_name)\n",
    "  model = LlavaForConditionalGeneration.from_pretrained(model_name, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True).to(\"cuda\", dtype=torch.bfloat16)\n",
    "  return model, processor\n",
    "\n",
    "def run_xtuner_llava(model, processor, user_image, user_text=None):\n",
    "  if user_text == None:\n",
    "    user_text = \"Describe the image using only 8 nouns. Focus on architecture and urbanism aspects.\"\n",
    "\n",
    "  prompt = f\"<|user|>\\n<image>\\n{user_text}<|end|>\\n<|assistant|>\\n\"\n",
    "  inputs = processor(images=user_image, text=prompt, return_tensors=\"pt\").to(\"cuda\", dtype=torch.bfloat16)\n",
    "  output = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n",
    "  caption = processor.decode(output[0], skip_special_tokens=True)\n",
    "  return caption\n",
    "\n",
    "  \n",
    "def prep_llava_model(model_name):\n",
    "  processor = AutoProcessor.from_pretrained(model_name)\n",
    "  model = LlavaNextForConditionalGeneration.from_pretrained(model_name, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True).to(\"cuda\", dtype=torch.bfloat16)\n",
    "  return model, processor\n",
    "\n",
    "def run_llava(model, processor, user_image, user_text=None):\n",
    "  if user_text == None:\n",
    "    user_text = \"Describe the image using only 8 nouns. Focus on architecture and urbanism aspects.\"\n",
    "\n",
    "  conversation = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "      {\"type\": \"text\", \"text\": user_text},\n",
    "      {\"type\": \"image\"},\n",
    "    ]\n",
    "  }]\n",
    "\n",
    "  prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "  inputs = processor(images=user_image, text=prompt, return_tensors=\"pt\").to(\"cuda\", dtype=torch.bfloat16)\n",
    "  output = model.generate(**inputs, max_new_tokens=100)\n",
    "  caption = processor.decode(output[0], skip_special_tokens=True)\n",
    "  return caption  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"llava-hf/llava-v1.6-vicuna-7b-hf\"\n",
    "\n",
    "model, processor = prep_llava_model(MODEL_NAME)\n",
    "caption = run_llava(model, processor, image)\n",
    "caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "\n",
    "model, processor = prep_llava_model(MODEL_NAME)\n",
    "caption = run_llava(model, processor, image)\n",
    "caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"xtuner/llava-llama-3-8b-transformers\"\n",
    "\n",
    "model, processor = prep_xtuner_llava_model(MODEL_NAME)\n",
    "caption = run_xtuner_llava(model, processor, image)\n",
    "caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"xtuner/llava-llama-3-8b-v1_1-transformers\"\n",
    "\n",
    "model, processor = prep_xtuner_llava_model(MODEL_NAME)\n",
    "caption = run_xtuner_llava(model, processor, image)\n",
    "caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"xtuner/llava-phi-3-mini-hf\"\n",
    "\n",
    "model, processor = prep_xtuner_llava_model(MODEL_NAME)\n",
    "caption = run_xtuner_llava(model, processor, image)\n",
    "caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.CPM2 import CPM2\n",
    "from models.CPM2_6 import CPM2_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CPM2.caption(image), CPM2_6.caption(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from PIL import Image as PImage\n",
    "\n",
    "from parameters import IMAGES_PATH\n",
    "\n",
    "io_file = \"10027.jpg\"\n",
    "io_file = \"10000.jpg\"\n",
    "\n",
    "input_file_path = path.join(IMAGES_PATH, io_file)\n",
    "image = PImage.open(input_file_path).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.Blip import Blip\n",
    "from models.Vit import Vit\n",
    "\n",
    "Blip.caption(image), Vit.caption(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### llama-cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "from os import listdir, path\n",
    "from PIL import Image as PImage\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "from llama_cpp.llama_chat_format import Llava15ChatHandler\n",
    "\n",
    "IMAGES_PATH = \"../../imgs/arquigrafia\"\n",
    "INPUT_FILES = sorted([f for f in listdir(IMAGES_PATH) if f.endswith(\"jpg\")])\n",
    "\n",
    "io_file = INPUT_FILES[10]\n",
    "io_file = \"10027.jpg\"\n",
    "io_file = \"10000.jpg\"\n",
    "io_file = \"12451.jpg\"\n",
    "\n",
    "input_file_path = path.join(IMAGES_PATH, io_file)\n",
    "image = PImage.open(input_file_path).convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_base64_data_uri(file_path):\n",
    "  with open(file_path, \"rb\") as img_file:\n",
    "    base64_data = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "    return f\"data:image/jpeg;base64,{base64_data}\"\n",
    "\n",
    "def prep_llama_cpp_model(model_name, model_file, proj_file):\n",
    "  model_path = hf_hub_download(model_name, filename=model_file)\n",
    "  proj_path = hf_hub_download(model_name, filename=proj_file)\n",
    "\n",
    "  chat_handler = Llava15ChatHandler(clip_model_path=proj_path)\n",
    "  llm = Llama(model_path=model_path, chat_handler=chat_handler, verbose=False, n_ctx=4096, n_threads=8, n_gpu_layers=-1, logits_all=True)\n",
    "  #llm = Llama.from_pretrained(repo_id=MODEL_NAME,filename=MODEL_FILE,verbose=False, n_ctx=4096, n_threads=8, n_gpu_layers=-1)\n",
    "  return llm\n",
    "\n",
    "def run_llama_cpp(llm, user_image_path, user_text=None):\n",
    "  user_image_uri = image_to_base64_data_uri(user_image_path)\n",
    "  if user_text == None:\n",
    "    user_text = \"Describe the image using only 8 nouns. Focus on architecture and urbanism aspects.\"\n",
    "\n",
    "  messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are an assistant who perfectly describes images using only nouns.\"},\n",
    "    {\"role\": \"user\", \"content\": [\n",
    "      {\"type\": \"image_url\", \"image_url\": {\"url\":  user_image_uri }},\n",
    "      {\"type\" : \"text\", \"text\": user_text}\n",
    "    ]}\n",
    "  ]\n",
    "  res = llm.create_chat_completion(messages=messages, max_tokens=60, stop=[\"</s>\"], top_k=1)\n",
    "  return res[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"xtuner/llava-llama-3-8b-v1_1-gguf\"\n",
    "MODEL_FILE = \"llava-llama-3-8b-v1_1-int4.gguf\"\n",
    "PROJ_FILE = \"llava-llama-3-8b-v1_1-mmproj-f16.gguf\"\n",
    "\n",
    "llm = prep_llama_cpp_model(MODEL_NAME, MODEL_FILE, PROJ_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = run_llama_cpp(llm, input_file_path)\n",
    "caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.GPT4 import GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPT4.caption(\"10026\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://platform.openai.com/docs/guides/vision\n",
    "# https://github.com/openai/openai-python\n",
    "\n",
    "from openai import OpenAI\n",
    "from msecrets import OPENAI_API_KEY\n",
    "\n",
    "client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  #model=\"gpt-4o-mini\",\n",
    "  model=\"gpt-4o-2024-08-06\",\n",
    "  messages=[{\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\"type\": \"text\", \"text\": \"What’s in this image? Answer using only nouns. Answer in english and portuguese.\"},\n",
    "        {\"type\": \"text\", \"text\": \"Separate english and portuguese descriptions with the word SEPARATOR\"},\n",
    "        {\"type\": \"image_url\",\n",
    "          \"image_url\": {\"url\": \"https://www.arquigrafia.org.br/arquigrafia-images/10026_view.jpg\",},\n",
    "        },\n",
    "      ],\n",
    "    }],\n",
    "  max_tokens=200,\n",
    ")\n",
    "\n",
    "print(response.choices[0])\n",
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TEST: Binaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from os import listdir, path\n",
    "from PIL import Image as PImage\n",
    "\n",
    "IMAGES_PATH = \"../../imgs/arquigrafia\"\n",
    "INPUT_FILES = sorted([f for f in listdir(IMAGES_PATH) if f.endswith(\"jpg\")])\n",
    "\n",
    "io_file = INPUT_FILES[100]\n",
    "input_file_path = path.join(IMAGES_PATH, io_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BINARIES = [\n",
    "  [\"horizontal\", \"vertical\"],\n",
    "  [\"translucent\", \"opaque\"],\n",
    "  [\"symmetric\", \"asymmetric\"],\n",
    "  [\"complex\", \"simple\"],\n",
    "  [\"internal\", \"external\"],\n",
    "  [\"open\", \"closed\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin2float(b, b0, b1):\n",
    "  if b0 in b:\n",
    "    return 0.0\n",
    "  elif b1 in b:\n",
    "    return 1.0\n",
    "  elif \"neither\" in b:\n",
    "    return 0.5\n",
    "  else:\n",
    "    print(\"b2f error: \", b)\n",
    "    return 0.5\n",
    "\n",
    "def run_binaries(img, model, bins):\n",
    "  bin_results = {}\n",
    "  for b in bins:\n",
    "    chat = [{\n",
    "      \"role\": \"user\",\n",
    "      \"content\": f\"Is the architecture pictured in the image more {b[0]}, {b[1]} or neither? Answer using only the words {b[0]}, {b[1]} or neither\"\n",
    "    }]\n",
    "    response, _, _ = model[\"model\"].chat(\n",
    "      image=img,\n",
    "      msgs=chat,\n",
    "      max_length=4,\n",
    "      context=None,\n",
    "      tokenizer=model[\"pre\"],\n",
    "      sampling=True,\n",
    "      temperature=0.005\n",
    "    )\n",
    "\n",
    "    response_list = ' '.join(response.split()).split()\n",
    "    if len(response_list) != 1:\n",
    "      print(\"wtf\", response_list)\n",
    "    bin_results[\"/\".join(b)] = bin2float(response_list[0].lower(), b[0], b[1])\n",
    "\n",
    "  return bin_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "image = PImage.open(input_file_path).convert(\"RGB\")\n",
    "image_binaries = run_binaries(image, CAP_MODEL, BINARIES)\n",
    "\n",
    "display(image)\n",
    "image_binaries"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
